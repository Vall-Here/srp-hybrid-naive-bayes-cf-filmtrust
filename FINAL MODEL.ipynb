{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03509a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from math import prod\n",
    "from decimal import Decimal, getcontext\n",
    "getcontext().prec = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60309d",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552ebc5",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fec2e",
   "metadata": {},
   "source": [
    "## Compute User & item prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f355ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_priors(ratings,plausible_rating, alpha=0.01, R=8):\n",
    "    num_users = len(ratings)\n",
    "    num_items = len(ratings[0])\n",
    "    # rating_values = list(range(1, R + 1))\n",
    "    rating_values = plausible_rating\n",
    "\n",
    "    prior_userbased = [[0 for _ in range(num_items)] for _ in rating_values]\n",
    "    prior_itembased = [[0 for _ in range(num_users)] for _ in rating_values]\n",
    "\n",
    "    y_index = 0\n",
    "    for y in rating_values:\n",
    "        y_index = y_index\n",
    "\n",
    "        # Prior user-based (per item j)\n",
    "        for j in range(num_items):\n",
    "            count_y = 0\n",
    "            count_nonzero = 0\n",
    "            for u in range(num_users):\n",
    "                r = ratings[u][j]   \n",
    "                if r != 0:\n",
    "                    count_nonzero += 1\n",
    "                    if r == y:\n",
    "                        count_y += 1\n",
    "            prior_userbased[y_index][j] = (count_y + alpha) / (count_nonzero + alpha * R)\n",
    "\n",
    "        # Prior item-based (per user u)\n",
    "        for u in range(num_users):\n",
    "            count_y = 0\n",
    "            count_nonzero = 0\n",
    "            for j in range(num_items):\n",
    "                r = ratings[u][j]\n",
    "                if r != 0:\n",
    "                    count_nonzero += 1\n",
    "                    if r == y:\n",
    "                        count_y += 1\n",
    "            prior_itembased[y_index][u] = (count_y + alpha) / (count_nonzero + alpha * R)\n",
    "        y_index = y_index + 1\n",
    "\n",
    "    return prior_userbased, prior_itembased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0af15",
   "metadata": {},
   "source": [
    "## Compute likelihood User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_userbased(ratings, u, i, y, alpha=0.01, R=8):\n",
    "    num_users = len(ratings)\n",
    "    num_items = len(ratings[0])\n",
    "    Iu = [j for j in range(num_items) if j != i and ratings[u][j] != 0]\n",
    "    # prob_list = []\n",
    "    product = Decimal(1.0)\n",
    "\n",
    "    for j in Iu:\n",
    "        k = ratings[u][j]\n",
    "        count_joint = 0\n",
    "        count_cond = 0\n",
    "        for v in range(num_users):\n",
    "            if ratings[v][i] == y:\n",
    "                if ratings[v][j] != 0:\n",
    "                    count_cond += 1\n",
    "                    if ratings[v][j] == k:\n",
    "                        count_joint += 1\n",
    "        prob = (count_joint + alpha) / (count_cond + alpha * R)\n",
    "        product *= Decimal(prob)\n",
    "        # prob_list.append(prob)\n",
    "    # print(\"======\")\n",
    "    # print(product, end=\"\\n\\n\")\n",
    "\n",
    "    return product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702342e",
   "metadata": {},
   "source": [
    "## Compute likelihood Item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ae839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_itembased(ratings, u, i, y, alpha=0.01, R=8):\n",
    "    num_users = len(ratings)\n",
    "    num_items = len(ratings[0])\n",
    "    Ui = [v for v in range(num_users) if v != u and ratings[v][i] != 0]\n",
    "    # prob_list = []\n",
    "    product = Decimal(1.0)\n",
    "\n",
    "    for v in Ui:\n",
    "        k = ratings[v][i]\n",
    "        count_joint = 0\n",
    "        count_cond = 0\n",
    "        for j in range(num_items):\n",
    "            if ratings[u][j] == y:\n",
    "                if ratings[v][j] != 0:\n",
    "                    count_cond += 1\n",
    "                    if ratings[v][j] == k:\n",
    "                        count_joint += 1\n",
    "        prob = (count_joint + alpha) / (count_cond + alpha * R)\n",
    "        product *= Decimal(prob)\n",
    "        # print(prob)\n",
    "        # print(\"======\")\n",
    "        # prob_list.append(prob)\n",
    "\n",
    "    return product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df3ad8",
   "metadata": {},
   "source": [
    "# Predict Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30574ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(ratings, u, i, prior_userbased, prior_itembased,plausible_rating, alpha=0.01, mode='hybrid'):\n",
    "    scores = []\n",
    "    # all_likelihood_user = []  \n",
    "    # all_likelihood_item = []\n",
    "    all_combined = []\n",
    "    R = len(plausible_rating)  \n",
    "\n",
    "    y_index = 0\n",
    "    for y in plausible_rating:\n",
    "        # print(y_index)\n",
    "        prior_user = prior_userbased[y_index][i]\n",
    "        prior_item = prior_itembased[y_index][u]\n",
    "\n",
    "      \n",
    "        # print(likelihood_item)\n",
    "\n",
    "        # all_likelihood_user.append(likelihood_user)\n",
    "        # all_likelihood_item.append(likelihood_item)\n",
    "\n",
    "        if mode == 'user':\n",
    "            likelihood_user = compute_likelihood_userbased(ratings, u, i, y, alpha, R)\n",
    "            score = Decimal(prior_user) * likelihood_user\n",
    "        elif mode == 'item':\n",
    "            likelihood_item = compute_likelihood_itembased(ratings, u, i, y, alpha, R)\n",
    "            score = Decimal(prior_item) * likelihood_item\n",
    "        else:  # hybrid\n",
    "            \n",
    "            \n",
    "            likelihood_user = compute_likelihood_userbased(ratings, u, i, y, alpha, R)\n",
    "            likelihood_item = compute_likelihood_itembased(ratings, u, i, y, alpha, R)\n",
    "            \n",
    "            \n",
    "            len_Iu = sum(1 for j in range(len(ratings[0])) if ratings[u][j] != 0 and j != i)\n",
    "            len_Ui = sum(1 for v in range(len(ratings)) if v != u and ratings[v][i] != 0)\n",
    "\n",
    "            score_user = (Decimal(prior_user) * likelihood_user) ** Decimal(1 / (1 + len_Iu)) if len_Iu > 0 else 0\n",
    "            score_item = (Decimal(prior_user) * likelihood_user) ** Decimal(1 / (1 + len_Ui)) if len_Ui > 0 else 0\n",
    "\n",
    "            score = score_user * score_item\n",
    "\n",
    "        scores.append(score)\n",
    "        all_combined.append(score)\n",
    "        y_index += 1\n",
    "\n",
    "    # predicted_rating = scores.index(max(scores)) + 1\n",
    "    predicted_rating = plausible_rating[scores.index(max(scores))]\n",
    "\n",
    "    return predicted_rating, {\n",
    "        'scores': scores,\n",
    "        # 'likelihood_user': all_likelihood_user,\n",
    "        # 'likelihood_item': all_likelihood_item,\n",
    "        'combined_score': all_combined\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73075211",
   "metadata": {},
   "source": [
    "## Load Data Full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7ce27",
   "metadata": {},
   "source": [
    "Untuk justifikasi tain test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_filmtrust_full(path):\n",
    "    df = pd.read_csv(path, sep=' ', engine='python', names=['user', 'item', 'rating'])\n",
    "\n",
    "    num_users = df['user'].nunique()\n",
    "    num_items = df['item'].nunique()\n",
    "\n",
    "    user_map = {uid: idx for idx, uid in enumerate(df['user'].unique())}\n",
    "    item_map = {iid: idx for idx, iid in enumerate(df['item'].unique())}\n",
    "\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for _, row in df.iterrows():\n",
    "        u = user_map[row['user']]\n",
    "        i = item_map[row['item']]\n",
    "        ratings[u][i] = row['rating']\n",
    "\n",
    "    return ratings\n",
    "\n",
    "ratings_full = load_filmtrust_full(\"./film-trust/ratings.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46868fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ecb48",
   "metadata": {},
   "source": [
    "# Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c437236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c06d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the original ratings.txt file\n",
    "# df = pd.read_csv('./film-trust/ratings.txt', sep=' ', names=['user', 'item', 'rating'])\n",
    "\n",
    "# print(f\"Dataset loaded: {len(df)} ratings\")\n",
    "# print(f\"Users: {df['user'].nunique()}\")\n",
    "# print(f\"Items: {df['item'].nunique()}\")\n",
    "# print(f\"Rating range: {df['rating'].min()} - {df['rating'].max()}\")\n",
    "\n",
    "# # Split into train (80%) and test (20%)\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"\\nTrain set: {len(train_df)} ratings ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "# print(f\"Test set: {len(test_df)} ratings ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# # Save train and test sets\n",
    "# train_df.to_csv('./film-trust/train.txt', sep=' ', header=False, index=False)\n",
    "# test_df.to_csv('./film-trust/test.txt', sep=' ', header=False, index=False)\n",
    "\n",
    "# print(\"\\nFiles saved:\")\n",
    "# print(\"- train.txt\")\n",
    "# print(\"- test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('./film-trust/ratings.txt', sep=' ', names=['user', 'item', 'rating'])\n",
    "user_ids = full_df['user'].unique()\n",
    "item_ids = full_df['item'].unique()\n",
    "user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "item_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
    "\n",
    "# Split dataset\n",
    "full_df['user_idx'] = full_df['user'].map(user_map)\n",
    "full_df['item_idx'] = full_df['item'].map(item_map)\n",
    "train_df, test_df = train_test_split(full_df[['user_idx', 'item_idx', 'rating']], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_df)} ratings ({len(train_df)/len(full_df)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} ratings ({len(test_df)/len(full_df)*100:.1f}%)\")\n",
    "train_df.to_csv('./film-trust/train.txt', sep=' ', header=False, index=False)\n",
    "test_df.to_csv('./film-trust/test.txt', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb18aa",
   "metadata": {},
   "source": [
    "## Get Plausible Ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"./film-trust/ratings.txt\", sep=' ', engine='python', names=['rating'])\n",
    "plausible_rating = temp_df['rating'].unique()\n",
    "plausible_rating.sort()\n",
    "plausible_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "pR = plausible_rating\n",
    "num_r = len(pR)\n",
    "num_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b9e86",
   "metadata": {},
   "source": [
    "## Load train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_filmtrust_train_make_matrix(path, full_ratings):\n",
    "#     df = pd.read_csv(path, sep=' ', engine='python', names=['user', 'item', 'rating'])\n",
    "#     df_full = pd.read_csv(full_ratings, sep=' ', engine='python', names=['user', 'item', 'rating'])\n",
    "\n",
    "#     num_users = df_full['user'].nunique()\n",
    "#     num_items = df_full['item'].nunique()\n",
    "\n",
    "#     user_map = {uid: idx for idx, uid in enumerate(df['user'].unique())}\n",
    "#     item_map = {iid: idx for idx, iid in enumerate(df['item'].unique())}\n",
    "\n",
    "#     ratings = np.zeros((num_users, num_items))\n",
    "#     for _, row in df.iterrows():\n",
    "#         u = user_map[row['user']]\n",
    "#         i = item_map[row['item']]\n",
    "#         ratings[u][i] = row['rating']\n",
    "\n",
    "#     return ratings, user_map, item_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_train, user_map, item_map = load_filmtrust_train_make_matrix(\"./film-trust/train.txt\",\"./film-trust/ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7639d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filmtrust_train_make_matrix(path, user_map, item_map):\n",
    "    df = pd.read_csv(path, sep=' ', engine='python', names=['user_idx', 'item_idx', 'rating'])\n",
    "\n",
    "    num_users = len(user_map)\n",
    "    num_items = len(item_map)\n",
    "\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for _, row in df.iterrows():\n",
    "        u = int(row['user_idx'])  # pastikan integer\n",
    "        i = int(row['item_idx'])  # pastikan integer\n",
    "        ratings[u][i] = row['rating']\n",
    "\n",
    "    return ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edbc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = load_filmtrust_train_make_matrix('./film-trust/train.txt', user_map, item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c69448",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_test_ratings(path):\n",
    "#     data = np.loadtxt(path, dtype={'names': ('u', 'i', 'r'), 'formats': (int, int, float)})\n",
    "#     test = np.array(\n",
    "#         [(int(row[0])-1, int(row[1])-1, float(row[2])) for row in data],\n",
    "#         dtype=[('u', int), ('i', int), ('r', float)]\n",
    "#     )\n",
    "#     return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_ratings(path):\n",
    "    df = pd.read_csv(path, sep=' ', names=['user_idx', 'item_idx', 'rating'])\n",
    "    return df.to_records(index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_test_ratings(\"./film-trust/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_index = 0\n",
    "# all_likelihood_user = []\n",
    "# all_likelihood_item = []\n",
    "# for y in pR:\n",
    "#     likelihood_user = compute_likelihood_userbased(ratings_train, 0, 10, y, 0.01, 8)\n",
    "#     likelihood_item = compute_likelihood_itembased(ratings_train, 0, 10, y, 0.01, 8)\n",
    "#     all_likelihood_user.append(likelihood_user)\n",
    "#     all_likelihood_item.append(likelihood_item)\n",
    "#     y_index = y_index + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb1b4b",
   "metadata": {},
   "source": [
    "## Precompute User & Item Prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prior_userbased, prior_itembased = compute_priors(ratings_train,pR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b315445",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_userbased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_itembased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred, _ = predict_rating(ratings_train, 0, 7, prior_userbased, prior_itembased,plausible_rating= pR, mode='hybrid')\n",
    "# _\n",
    "# pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593bf91",
   "metadata": {},
   "source": [
    "## Loop prediction for each data test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prediksi dan evaluasi\n",
    "from tqdm import tqdm\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for u, i, actual in tqdm(test_set):\n",
    "    pred, _ = predict_rating(ratings_train, u, i, prior_userbased, prior_itembased,plausible_rating= pR, mode='hybrid')\n",
    "    y_true.append(actual)\n",
    "    y_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3607e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00161fd1",
   "metadata": {},
   "source": [
    "## Export to evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c602ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "df_results.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00519cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(test_set)\n",
    "df_test.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5281f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_full = pd.DataFrame(ratings_full)\n",
    "df_full.to_csv('ratings_full.csv', index=False)\n",
    "\n",
    "df_train = pd.DataFrame(ratings_train)\n",
    "df_train.to_csv('ratings_train.csv', index=False)\n",
    "\n",
    "df_test = pd.DataFrame(test_set)\n",
    "df_test.to_csv('test_set.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior_userbased = pd.DataFrame(prior_userbased)\n",
    "df_prior_userbased.to_csv('prior_userbased.csv', index=False)\n",
    "\n",
    "df_prior_itembased = pd.DataFrame(prior_itembased)\n",
    "df_prior_itembased.to_csv('prior_itembased.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
